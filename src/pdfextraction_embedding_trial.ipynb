{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.vectorstores.faiss.FAISS object at 0x000002423B527640>\n",
      " KFM Kingdom Holdings Limited and Hong Kong Exchanges and Clearing Limited and The Stock Exchange of Hong Kong Limited.\n",
      " No, I cannot. This announcement does not provide the stock details for the mentioned participants in the transaction.\n",
      " The announcement date of the transaction was 13 August 2021.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from pytesseract import image_to_string\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pypdfium2 as pdfium\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-....\"\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert_pdf_to_images(self, file_path, scale=300 / 72):\n",
    "        pdf_file = pdfium.PdfDocument(file_path)\n",
    "        page_indices = [i for i in range(len(pdf_file))]\n",
    "        renderer = pdf_file.render(\n",
    "            pdfium.PdfBitmap.to_pil,\n",
    "            page_indices=page_indices,\n",
    "            scale=scale,\n",
    "        )\n",
    "        final_images = []\n",
    "        for i, image in zip(page_indices, renderer):\n",
    "            image_byte_array = BytesIO()\n",
    "            image.save(image_byte_array, format='jpeg', optimize=True)\n",
    "            image_byte_array = image_byte_array.getvalue()\n",
    "            final_images.append(dict({i: image_byte_array}))\n",
    "        return final_images\n",
    "\n",
    "    def extract_text_from_img(self, list_dict_final_images):\n",
    "        image_list = [list(data.values())[0] for data in list_dict_final_images]\n",
    "        image_content = []\n",
    "        for index, image_bytes in enumerate(image_list):\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "            raw_text = str(image_to_string(image))\n",
    "            image_content.append(raw_text)\n",
    "        return \"\\\\n\".join(image_content)\n",
    "\n",
    "    def extract_content_from_url(self, url):\n",
    "        images_list = self.convert_pdf_to_images(url)\n",
    "        text_with_pytesseract = self.extract_text_from_img(images_list)\n",
    "        # print(text_with_pytesseract)\n",
    "        return text_with_pytesseract\n",
    "\n",
    "    \n",
    "    def process_documents(self, file_paths):\n",
    "        results = []\n",
    "        for file_path in file_paths:\n",
    "            content = self.extract_content_from_url(file_path)\n",
    "            text_splitter = CharacterTextSplitter(\n",
    "                separator = \"\\n\",\n",
    "                chunk_size = 1000,\n",
    "                chunk_overlap = 200,\n",
    "                length_function = len,\n",
    "            )\n",
    "            texts = text_splitter.split_text(content)\n",
    "\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "\n",
    "            docsearch = FAISS.from_texts(texts, embeddings)\n",
    "            print(docsearch)\n",
    "\n",
    "            chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n",
    "\n",
    "            query = \"What are the company that are involved in the deal?\"\n",
    "            docs = docsearch.similarity_search(query)\n",
    "            print(chain.run(input_documents=docs, question=query))\n",
    "\n",
    "            query = \"Can you identify the stock details details for the mentioned participants in the transaction if any?\"\n",
    "            docs = docsearch.similarity_search(query)\n",
    "            print(chain.run(input_documents=docs, question=query))\n",
    "\n",
    "            query = \"Can you extract the announcement date of the transaction?\"\n",
    "            docs = docsearch.similarity_search(query)\n",
    "            print(chain.run(input_documents=docs, question=query))\n",
    "\n",
    "            # # embeddings = OpenAIEmbeddings()\n",
    "            # openai.api_key = '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'\n",
    "\n",
    "            # response = openai.Embeddable.create(\n",
    "            #     model=\"text-davinci-002\",\n",
    "            #     embeddables=texts\n",
    "            # )\n",
    "\n",
    "            # embeddings = response['embeddings']\n",
    "\n",
    "            # import pinecone\n",
    "\n",
    "            # pinecone.init(api_key=openai.api_key)\n",
    "\n",
    "            # # Create an index\n",
    "            # pinecone.create_index(index_name=\"embeddings\", metric=\"cosine\", shards=1)\n",
    "\n",
    "            # # Uploading vectors\n",
    "            # index = pinecone.Index(index_name=\"embeddings\")\n",
    "            # ids = [\"entity_id_1\"]\n",
    "            # vectors = [embeddings]   \n",
    "            # index.upsert(vectors=zip(ids, vectors))\n",
    "\n",
    "            # # ... you can then use this index for future similarity/search operations\n",
    "\n",
    "            # pinecone.deinit()\n",
    "\n",
    "\n",
    "            # prompt = \"What are the company that are involved in the deal?\"\"\n",
    "            # response = openai.Embeddable.create(\n",
    "            #     model=\"text-davinci-002\",\n",
    "            #     embeddables=[prompt]\n",
    "            # )\n",
    "            # prompt_embedding = response['embeddings']\n",
    "\n",
    "            # Search in the Pinecone index for most similar embeddings\n",
    "\n",
    "            # pinecone.init(api_key=openai.api_key)\n",
    "            # index = pinecone.Index(index_name=\"embeddings\")\n",
    "\n",
    "            # # Vector of the new message\n",
    "            # query_vector = [prompt_embedding]\n",
    "            # # Query Pinecone for the 5 most similar vectors\n",
    "            # top_results = index.query(queries=query_vector, top_k=5)\n",
    "\n",
    "            # print(top_results)  \n",
    "\n",
    "            # pinecone.deinit()\n",
    "\n",
    "            # if isinstance(data, list):\n",
    "            #     results.extend(content)\n",
    "            # else:\n",
    "            #     results.append(content)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    document_processor = DocumentProcessor()\n",
    "    uploaded_files_paths = [r'Docs\\PEVC.pdf']\n",
    "    processed_results = document_processor.process_documents(uploaded_files_paths)\n",
    "    print(processed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\apadmanabh02\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\apadmanabh02\\Anaconda3\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
